{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ff6RhfDHhLXO"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vWdmJrDZhLXO"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from __future__ import print_function\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Dropout, SimpleRNN, LSTM\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "x4nOpybDhLXO"
   },
   "outputs": [],
   "source": [
    "# The url where the data is held\n",
    "url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
    "\n",
    "# Reading in the data and encoding it\n",
    "r = requests.get(url)\n",
    "r.encoding = r.apparent_encoding\n",
    "data = r.text\n",
    "data = data.split('\\r\\n')\n",
    "toc = [l.strip() for l in data[44:130:2]]\n",
    "\n",
    "# Skip the Table of Contents\n",
    "data = data[135:]\n",
    "\n",
    "# Fixing Titles\n",
    "toc[9] = 'THE LIFE OF KING HENRY V'\n",
    "toc[18] = 'MACBETH'\n",
    "toc[24] = 'OTHELLO, THE MOOR OF VENICE'\n",
    "toc[34] = 'TWELFTH NIGHT: OR, WHAT YOU WILL'\n",
    "\n",
    "locations = {id_:{'title':title, 'start':-99} for id_,title in enumerate(toc)}\n",
    "\n",
    "# Start \n",
    "for e,i in enumerate(data):\n",
    "    for t,title in enumerate(toc):\n",
    "        if title in i:\n",
    "            locations[t].update({'start':e})\n",
    "            \n",
    "# Turn into a DataFrame from a dictionary\n",
    "df_toc = pd.DataFrame.from_dict(locations, orient='index')\n",
    "df_toc['end'] = df_toc['start'].shift(-1).apply(lambda x: x-1)\n",
    "df_toc.loc[42, 'end'] = len(data)\n",
    "df_toc['end'] = df_toc['end'].astype('int')\n",
    "\n",
    "df_toc['text'] = df_toc.apply(lambda x: '\\r\\n'.join(data[ x['start'] : int(x['end']) ]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "JWmGxj1GhLXO",
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputId": "b1df99d9-e5c9-4e1c-d448-62538e20e420"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                      title  start    end  \\\n0       THE TRAGEDY OF ANTONY AND CLEOPATRA    -99  14379   \n1                            AS YOU LIKE IT  14380  17171   \n2                      THE COMEDY OF ERRORS  17172  20372   \n3                 THE TRAGEDY OF CORIOLANUS  20373  30346   \n4                                 CYMBELINE  30347  30364   \n5  THE TRAGEDY OF HAMLET, PRINCE OF DENMARK  30365  37051   \n6   THE FIRST PART OF KING HENRY THE FOURTH  37052  41767   \n7  THE SECOND PART OF KING HENRY THE FOURTH  41768   -100   \n\n                                                text  \n0                                                     \n1  AS YOU LIKE IT\\r\\n\\r\\n\\r\\nDRAMATIS PERSONAE.\\r...  \n2  THE COMEDY OF ERRORS\\r\\n\\r\\n\\r\\n\\r\\nContents\\r...  \n3  THE TRAGEDY OF CORIOLANUS\\r\\n\\r\\nDramatis Pers...  \n4  CYMBELINE.\\r\\nLaud we the gods;\\r\\nAnd let our...  \n5  THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\\r\\n\\r...  \n6  THE FIRST PART OF KING HENRY THE FOURTH\\r\\n\\r\\...  \n7  THE SECOND PART OF KING HENRY THE FOURTH\\r\\n\\r...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>start</th>\n      <th>end</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>THE TRAGEDY OF ANTONY AND CLEOPATRA</td>\n      <td>-99</td>\n      <td>14379</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AS YOU LIKE IT</td>\n      <td>14380</td>\n      <td>17171</td>\n      <td>AS YOU LIKE IT\\r\\n\\r\\n\\r\\nDRAMATIS PERSONAE.\\r...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>THE COMEDY OF ERRORS</td>\n      <td>17172</td>\n      <td>20372</td>\n      <td>THE COMEDY OF ERRORS\\r\\n\\r\\n\\r\\n\\r\\nContents\\r...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>THE TRAGEDY OF CORIOLANUS</td>\n      <td>20373</td>\n      <td>30346</td>\n      <td>THE TRAGEDY OF CORIOLANUS\\r\\n\\r\\nDramatis Pers...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CYMBELINE</td>\n      <td>30347</td>\n      <td>30364</td>\n      <td>CYMBELINE.\\r\\nLaud we the gods;\\r\\nAnd let our...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>THE TRAGEDY OF HAMLET, PRINCE OF DENMARK</td>\n      <td>30365</td>\n      <td>37051</td>\n      <td>THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\\r\\n\\r...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>THE FIRST PART OF KING HENRY THE FOURTH</td>\n      <td>37052</td>\n      <td>41767</td>\n      <td>THE FIRST PART OF KING HENRY THE FOURTH\\r\\n\\r\\...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>THE SECOND PART OF KING HENRY THE FOURTH</td>\n      <td>41768</td>\n      <td>-100</td>\n      <td>THE SECOND PART OF KING HENRY THE FOURTH\\r\\n\\r...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shakespeare Data Parsed by Play\n",
    "print(df_toc.shape)\n",
    "df_toc.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "id": "nbV8OQHa05Oo",
    "outputId": "fb82cf54-1529-4db6-8d00-9d44de533288"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'THE TRAGEDY OF HAMLET, PRINCE OF DENMARK    Contents  ACT I Scene I. Elsinore. A platform before the Castle. Scene II. Elsinore. A room of state in the Castle Scene III. A room in Polonius’s house. Scene IV. The platform. Scene V. A more remote part '"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the text\n",
    "df_toc['text'] = [i.replace('\\r\\n', ' ') for i in df_toc['text']]\n",
    "df_toc['text'] = [i.replace('[', ' ') for i in df_toc['text']]\n",
    "df_toc['text'] = [i.replace(']', ' ') for i in df_toc['text']]\n",
    "df_toc['text'] = [i.replace('-', ' ') for i in df_toc['text']]\n",
    "df_toc['text'] = [i.replace('_', ' ') for i in df_toc['text']]\n",
    "df_toc['text'] = [i.replace('—', ' ') for i in df_toc['text']]\n",
    "df_toc['text'] = [i.replace('|', ' ') for i in df_toc['text']]\n",
    "df_toc['text'] = [i.replace('`', ' ') for i in df_toc['text']]\n",
    "df_toc['text'] = [i.replace('}', ' ') for i in df_toc['text']]\n",
    "df_toc['text'] = [i.replace('{', ' ') for i in df_toc['text']]\n",
    "df_toc['text'] = [i.replace('(', ' ') for i in df_toc['text']]\n",
    "df_toc['text'] = [i.replace(')', ' ') for i in df_toc['text']]\n",
    "df_toc['text'] = [i.replace('\\\\', ' ') for i in df_toc['text']]\n",
    "df_toc['text'] = [i.replace('*', ' ') for i in df_toc['text']]\n",
    "df_toc['text'] = [i.replace('/', ' ') for i in df_toc['text']]\n",
    "df_toc['text'] = [i.replace('%', ' ') for i in df_toc['text']]\n",
    "df_toc['text'] = [i.replace('Æ', ' ') for i in df_toc['text']]\n",
    "df_toc['text'] = [i.replace('æ', ' ') for i in df_toc['text']]\n",
    "df_toc['text'] = [i.replace('$', ' ') for i in df_toc['text']]\n",
    "df_toc['text'] = [i.replace('œ', ' ') for i in df_toc['text']]\n",
    "df_toc['text'] = [i.replace(':', ' ') for i in df_toc['text']]\n",
    "df_toc['text'] = [i.replace(';', ' ') for i in df_toc['text']]\n",
    "df_toc['text'] = [i.replace('@', ' ') for i in df_toc['text']]\n",
    "\n",
    "# Look at one of the text values to check work\n",
    "df_toc['text'][5][:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "id": "cyk3UW-N05NM",
    "outputId": "ba2d656e-5ac6-4b81-c831-f5810e5438ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'THE TRAGEDY OF HAMLET, PRINCE OF DENMARK Contents ACT I Scene I. Elsinore. A platform before the Castle. Scene II. Elsinore. A room of state in the Castle Scene III. A room in Polonius’s house. Scene IV. The platform. Scene V. A more remote part of t'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove extra spacing in the text\n",
    "df_toc['text'] = [re.sub('\\s\\s+', ' ', i) for i in df_toc.text]\n",
    "\n",
    "# Look at one of the text values to check work\n",
    "df_toc['text'][5][:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rhOOuQyE05K2",
    "outputId": "72c55655-34a0-4c6e-8e7d-1d226b6f20ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My corpus contains 82 unique characters.\n"
     ]
    }
   ],
   "source": [
    "# Character encoding       \n",
    "# Tried/failed `text = [df_toc['text'][i] for i in range(len(df_toc['text']))]`\n",
    "text = ' '.join(df_toc['text'])  # Don't need this, done above\n",
    "chars = list(set(text))\n",
    "\n",
    "char_int = {c:i for i, c in enumerate(chars)}\n",
    "int_char = {i:c for i, c in enumerate(chars)}\n",
    "\n",
    "print(f'My corpus contains {len(chars)} unique characters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9Ub9VYD05Ir",
    "outputId": "da8535e6-b90a-4d3c-8792-5f15da6f4c4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['S',\n 'u',\n 'A',\n 'p',\n 'f',\n 'r',\n 'v',\n 'g',\n 's',\n 'W',\n 'N',\n '\"',\n 'y',\n \"'\",\n 'k',\n 'o',\n '?',\n 'Q',\n '5',\n 'e',\n 'I',\n 'b',\n 'h',\n 'n',\n 'O',\n '1',\n 'î',\n 'Y',\n 'M',\n 'q',\n '.',\n '’',\n '4',\n 'B',\n '2',\n 'É',\n 'a',\n 'è',\n '”',\n '8',\n '6',\n 'K',\n 'H',\n '!',\n 'z',\n 'w',\n ' ',\n 'V',\n 'é',\n 'X',\n 'D',\n 'P',\n 'T',\n 'à',\n 'x',\n '‘',\n '&',\n 'l',\n 'G',\n 'U',\n 'd',\n 'ê',\n '9',\n '7',\n 'Z',\n 'E',\n 'c',\n ',',\n 'i',\n 'R',\n '0',\n 'ç',\n 'C',\n 'j',\n 't',\n 'J',\n '3',\n 'F',\n 'L',\n '“',\n 'm',\n 'â']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q81qFGmm05GY",
    "outputId": "0bf72dff-8855-4a5c-cdbf-93ea7029b25b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of sequences: 2787286\n"
     ]
    }
   ],
   "source": [
    "# Create the sequence data\n",
    "max_len = 150\n",
    "step = 5\n",
    "\n",
    "# Encode each character\n",
    "encoded = [char_int[c] for c in text]\n",
    "sequences = []  # Each element == max_len\n",
    "next_chars = []  # One element for each sequence\n",
    "\n",
    "for i in range(0, len(encoded) - max_len, step):\n",
    "    sequences.append(encoded[i : i + max_len])\n",
    "    next_chars.append(encoded[i + max_len])\n",
    "\n",
    "# Look at the count of sequences\n",
    "print(f'Length of sequences: {len(sequences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Sgfx0d7I05ET",
    "outputId": "9e2dde52-8af0-4806-f939-8f8b0831d69d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "' AS YOU LI'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the text\n",
    "text[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XkJ9xVTV05Bv",
    "outputId": "0f939f1b-b160-4cef-8da5-4a0368ae4632"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[46,\n 2,\n 0,\n 46,\n 27,\n 24,\n 59,\n 46,\n 78,\n 20,\n 41,\n 65,\n 46,\n 20,\n 52,\n 46,\n 50,\n 69,\n 2,\n 28,\n 2,\n 52,\n 20,\n 0,\n 46,\n 51,\n 65,\n 69,\n 0,\n 24,\n 10,\n 2,\n 65,\n 30,\n 46,\n 50,\n 59,\n 41,\n 65,\n 67,\n 46,\n 57,\n 68,\n 6,\n 68,\n 23,\n 7,\n 46,\n 68,\n 23,\n 46,\n 19,\n 54,\n 68,\n 57,\n 19,\n 46,\n 77,\n 69,\n 65,\n 50,\n 65,\n 69,\n 20,\n 72,\n 41,\n 67,\n 46,\n 22,\n 68,\n 8,\n 46,\n 21,\n 5,\n 15,\n 74,\n 22,\n 19,\n 5,\n 67,\n 46,\n 36,\n 23,\n 60,\n 46,\n 1,\n 8,\n 1,\n 5,\n 3,\n 19,\n 5,\n 46,\n 15,\n 4,\n 46,\n 22,\n 68,\n 8,\n 46,\n 60,\n 15,\n 80,\n 68,\n 23,\n 68,\n 15,\n 23,\n 8,\n 46,\n 2,\n 28,\n 20,\n 65,\n 10,\n 0,\n 67,\n 46,\n 57,\n 15,\n 5,\n 60,\n 46,\n 36,\n 74,\n 74,\n 19,\n 23,\n 60,\n 68,\n 23,\n 7,\n 46,\n 15,\n 23,\n 46,\n 74,\n 22,\n 19,\n 46,\n 21,\n 36,\n 23,\n 68,\n 8,\n 22,\n 19,\n 60,\n 46,\n 50]"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the sequences\n",
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4UR6Qn1_04_n",
    "outputId": "d4b6d706-4d0f-41cd-a285-e2f9508cd6d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the next chars\n",
    "next_chars[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "lyIhNfbm049c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2787286, 150, 82) (2787286, 82)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Specify my x(training data) and y (target)\n",
    "x = np.zeros((len(sequences), max_len, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i, t, char] = 1\n",
    "    y[i, next_chars[i]] = 1\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "3kccJcpw047N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 150, 512)          1218560   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 150, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 150, 512)          2099200   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 82)                42066     \n",
      "=================================================================\n",
      "Total params: 5,459,026\n",
      "Trainable params: 5,459,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build a single LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(max_len, len(chars)),\n",
    "               activation='sigmoid'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(512))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "# Compile my model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='nadam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Look at the summary of my model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "SxoaciJy0446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "   OOM when allocating tensor with shape[32,512] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node TensorArrayUnstack/TensorListFromTensor}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[sequential/lstm_1/PartitionedCall]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_7589]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mResourceExhaustedError\u001B[0m                    Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-29-f6c61de0d0f0>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# Fit my model\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m model.fit(x, y,\n\u001B[0m\u001B[0;32m      3\u001B[0m           \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m32\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m           epochs=15)\n",
      "\u001B[1;32mc:\\users\\sahmj\\.virtualenvs\\ds-unit-4-sprint-3-deep-learning\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001B[0m in \u001B[0;36m_method_wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    106\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_method_wrapper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    107\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_in_multi_worker_mode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 108\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    109\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    110\u001B[0m     \u001B[1;31m# Running inside `run_distribute_coordinator` already.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\sahmj\\.virtualenvs\\ds-unit-4-sprint-3-deep-learning\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1096\u001B[0m                 batch_size=batch_size):\n\u001B[0;32m   1097\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1098\u001B[1;33m               \u001B[0mtmp_logs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1099\u001B[0m               \u001B[1;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1100\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\sahmj\\.virtualenvs\\ds-unit-4-sprint-3-deep-learning\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    778\u001B[0m       \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    779\u001B[0m         \u001B[0mcompiler\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"nonXla\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 780\u001B[1;33m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    781\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    782\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_tracing_count\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\sahmj\\.virtualenvs\\ds-unit-4-sprint-3-deep-learning\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m_call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    838\u001B[0m         \u001B[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    839\u001B[0m         \u001B[1;31m# stateless function.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 840\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    841\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    842\u001B[0m       \u001B[0mcanon_args\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcanon_kwds\u001B[0m \u001B[1;33m=\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\sahmj\\.virtualenvs\\ds-unit-4-sprint-3-deep-learning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2827\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2828\u001B[0m       \u001B[0mgraph_function\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_maybe_define_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2829\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_filtered_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2830\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2831\u001B[0m   \u001B[1;33m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\sahmj\\.virtualenvs\\ds-unit-4-sprint-3-deep-learning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_filtered_call\u001B[1;34m(self, args, kwargs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1841\u001B[0m       \u001B[0;31m`\u001B[0m\u001B[0margs\u001B[0m\u001B[0;31m`\u001B[0m \u001B[1;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;31m`\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1842\u001B[0m     \"\"\"\n\u001B[1;32m-> 1843\u001B[1;33m     return self._call_flat(\n\u001B[0m\u001B[0;32m   1844\u001B[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001B[0;32m   1845\u001B[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001B[1;32mc:\\users\\sahmj\\.virtualenvs\\ds-unit-4-sprint-3-deep-learning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1921\u001B[0m         and executing_eagerly):\n\u001B[0;32m   1922\u001B[0m       \u001B[1;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1923\u001B[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001B[0m\u001B[0;32m   1924\u001B[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0;32m   1925\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001B[1;32mc:\\users\\sahmj\\.virtualenvs\\ds-unit-4-sprint-3-deep-learning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    543\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0m_InterpolateFunctionError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    544\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mcancellation_manager\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 545\u001B[1;33m           outputs = execute.execute(\n\u001B[0m\u001B[0;32m    546\u001B[0m               \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msignature\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    547\u001B[0m               \u001B[0mnum_outputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_num_outputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\sahmj\\.virtualenvs\\ds-unit-4-sprint-3-deep-learning\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     57\u001B[0m   \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     58\u001B[0m     \u001B[0mctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 59\u001B[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0m\u001B[0;32m     60\u001B[0m                                         inputs, attrs, num_outputs)\n\u001B[0;32m     61\u001B[0m   \u001B[1;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mResourceExhaustedError\u001B[0m:    OOM when allocating tensor with shape[32,512] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node TensorArrayUnstack/TensorListFromTensor}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[sequential/lstm_1/PartitionedCall]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_7589]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "# Fit my model\n",
    "model.fit(x, y,\n",
    "          batch_size=32,\n",
    "          epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vr67X8g042f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpmvnwsZ040W"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFCKZZQU04yC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "guTO14-Z04v-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TyRf07LL04tv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "po-Lt3GG04re"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55w2BTVT04mh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gRsP6RPQ04kE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-TdXv5B104hs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1G6UMVxc04fT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iiGT6zj004cr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yIszFeQH04aX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of LS_DS_431_RNN_and_LSTM_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nteract": {
   "version": "0.23.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}