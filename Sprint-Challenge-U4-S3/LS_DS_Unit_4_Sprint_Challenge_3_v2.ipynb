{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_Unit_4_Sprint_Challenge_3_v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernel_info": {
      "name": "u4-s3-dnn"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "nteract": {
      "version": "0.23.1"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63Dvb1qXePS4"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "# Major Neural Network Architectures Challenge\n",
        "## *Data Science Unit 4 Sprint 3 Challenge*\n",
        "\n",
        "In this sprint challenge, you'll explore some of the cutting edge of Data Science. This week we studied several famous neural network architectures: \n",
        "recurrent neural networks (RNNs), long short-term memory (LSTMs), convolutional neural networks (CNNs), and Autoencoders. In this sprint challenge, you will revisit these models. Remember, we are testing your knowledge of these architectures not your ability to fit a model with high accuracy. \n",
        "\n",
        "__*Caution:*__  these approaches can be pretty heavy computationally. All problems were designed so that you should be able to achieve results within at most 5-10 minutes of runtime locally, on AWS SageMaker, on Colab or on a comparable environment. If something is running longer, double check your approach!\n",
        "\n",
        "## Challenge Objectives\n",
        "*You should be able to:*\n",
        "* <a href=\"#p1\">Part 1</a>: Train a LSTM classification model\n",
        "* <a href=\"#p2\">Part 2</a>: Utilize a pre-trained CNN for object detection\n",
        "* <a href=\"#p3\">Part 3</a>: Describe a use case for an autoencoder\n",
        "* <a href=\"#p4\">Part 4</a>: Describe yourself as a Data Science and elucidate your vision of AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5UwGRnJOmD4"
      },
      "source": [
        "<a id=\"p1\"></a>\n",
        "## Part 1 - LSTMSs\n",
        "\n",
        "Use a LSTM to fit a multi-class classification model on Reuters news articles to distinguish topics of articles. The data is already encoded properly for use in a LSTM model. \n",
        "\n",
        "Your Tasks: \n",
        "- Use Keras to fit a predictive model, classifying news articles into topics. \n",
        "- Report your overall score and accuracy\n",
        "\n",
        "For reference, the [Keras IMDB sentiment classification example](https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py) will be useful, as well as the LSTM code we used in class.\n",
        "\n",
        "__*Note:*__  Focus on getting a running model, not on maxing accuracy with extreme data size or epoch numbers. Only revisit and push accuracy if you get everything else done!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS-9ksWjoJit"
      },
      "source": [
        "from tensorflow.keras.datasets import reuters\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=None,\n",
        "                                                         skip_top=0,\n",
        "                                                         maxlen=None,\n",
        "                                                         test_split=0.2,\n",
        "                                                         seed=723812,\n",
        "                                                         start_char=1,\n",
        "                                                         oov_char=2,\n",
        "                                                         index_from=3)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLKqFh8DovaN",
        "outputId": "12252f29-de0f-408a-ab20-35f33b3ffafd"
      },
      "source": [
        "# Demo of encoding\n",
        "\n",
        "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
        "\n",
        "print(f\"Iran is encoded as {word_index['iran']} in the data\")\n",
        "print(f\"London is encoded as {word_index['london']} in the data\")\n",
        "print(\"Words are encoded as numbers in our dataset.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iran is encoded as 779 in the data\n",
            "London is encoded as 544 in the data\n",
            "Words are encoded as numbers in our dataset.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QVSlFEAqWJM"
      },
      "source": [
        "# Do not change this line. You need the +1 for some reason. \n",
        "max_features = len(word_index.values()) + 1\n",
        "\n",
        "# TODO - your code! - Imports\n",
        "from __future__ import print_function\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "import random\n",
        "import sys\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.layers import Dense, Embedding, Dropout, LSTM\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AIA-1VstwBm",
        "outputId": "85753f78-34e8-4c3d-a57c-f243ee1e6101"
      },
      "source": [
        "# Print the number of samples for train and test sets\r\n",
        "print(f'Number of Training Samples: {len(X_train)}')\r\n",
        "print(f'Number of Test Samples: {len(X_test)}')\r\n",
        "\r\n",
        "# Print the number of classes\r\n",
        "num_class = max(y_train) + 1\r\n",
        "print(f'Number of Training Classes: {num_class}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Training Samples: 8982\n",
            "Number of Test Samples: 2246\n",
            "Number of Training Classes: 46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF8XbgyskEkI",
        "outputId": "af608e0d-c86a-43dd-856e-72e08fd24c48"
      },
      "source": [
        "# Look at one of the train data\r\n",
        "print(X_train[3])\r\n",
        "print(y_train[3])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 346, 273, 94, 187, 53, 74, 472, 26, 14, 46, 19, 124, 15, 39, 74, 32, 6582, 18, 14, 46, 61, 6097, 18, 1730, 1668, 32, 11, 14, 996, 12, 11, 123, 346, 39, 235, 627, 276, 5, 19, 19, 11, 15, 17, 12]\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8DMvcCSkzVQ"
      },
      "source": [
        "# Ensure that \"special\" words are mapped into human readable\r\n",
        "word_index = {k:(v + 3) for k, v in word_index.items()}\r\n",
        "word_index[\"<PAD>\"] = 0\r\n",
        "word_index[\"<START>\"] = 1\r\n",
        "word_index[\"<UNKNOWN>\"] = 2\r\n",
        "\r\n",
        "# Preform reverse word lookup and make it callable\r\n",
        "rev_word_index = dict([(value, key) for (key, value) in word_index.items()])\r\n",
        "def decode_review(text):\r\n",
        "  return ' '.join([rev_word_index.get(i, '?') for i in text])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQYW10qakzeq",
        "outputId": "ebbfae78-65c8-4e4f-b4c0-0b0e673d8513"
      },
      "source": [
        "# Look at the max, min, average length of the whole dataset\r\n",
        "all_articles = np.concatenate((X_train, X_test), axis=0)\r\n",
        "print(f'Maximum article length: {len(max((all_articles), key=len))}')\r\n",
        "print(f'Minimum article length: {len(min((all_articles), key=len))}')\r\n",
        "result = [len(x) for x in all_articles]\r\n",
        "print(f'Average article length: {round(np.mean(result))}')\r\n",
        "\r\n",
        "# Print an article and it's class as stored in the dataset\r\n",
        "class_names = ['Technology', 'Sports', 'Fashion']\r\n",
        "print('\\nA Machine Readable Article:')\r\n",
        "print('  Article Text: ' + str(X_train[9]))\r\n",
        "print('  Article Sentiment: ' + str(y_train[9]))\r\n",
        "\r\n",
        "# Print the same article in human readable format\r\n",
        "print('\\nA Human Readable Article:')\r\n",
        "print('  Article Text: ' + decode_review(X_train[9]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum article length: 2376\n",
            "Minimum article length: 2\n",
            "Average article length: 146.0\n",
            "\n",
            "A Machine Readable Article:\n",
            "  Article Text: [1, 53, 46, 312, 26, 14, 74, 134, 26, 39, 46, 5775, 18, 14, 74, 19, 3843, 18, 86, 981, 19, 11, 14, 924, 19, 11, 155, 230, 53, 74, 321, 26, 14, 74, 119, 26, 39, 74, 32, 5328, 18, 14, 74, 32, 3253, 18, 86, 2389, 44, 11, 14, 2012, 61, 11, 17, 12]\n",
            "  Article Sentiment: 3\n",
            "\n",
            "A Human Readable Article:\n",
            "  Article Text: <START> shr loss seven cts vs profit 12 cts net loss 662 000 vs profit 1 520 000 revs 59 1 mln vs 63 1 mln six mths shr profit 23 cts vs profit 20 cts net profit 2 802 000 vs profit 2 543 000 revs 138 5 mln vs 126 7 mln reuter 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2op0QvJzn85"
      },
      "source": [
        "# Add padding to the begining of the sequence\r\n",
        "max_len = 125\r\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_len)\r\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_len)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ts3i3GbU1Qnr",
        "outputId": "af7c632a-f186-4f27-f088-9195177a1579"
      },
      "source": [
        "X_train[3]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    1,  346,  273,   94,  187,   53,   74,  472,\n",
              "         26,   14,   46,   19,  124,   15,   39,   74,   32, 6582,   18,\n",
              "         14,   46,   61, 6097,   18, 1730, 1668,   32,   11,   14,  996,\n",
              "         12,   11,  123,  346,   39,  235,  627,  276,    5,   19,   19,\n",
              "         11,   15,   17,   12], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYKmPGZDgtCk",
        "outputId": "278e307b-39e8-48fd-8f68-c155fdb06d2e"
      },
      "source": [
        "from tensorflow.keras.layers import Bidirectional\r\n",
        "\r\n",
        "# Create a LSTM model\r\n",
        "lstm = Sequential()\r\n",
        "lstm.add(Embedding(max_features, 64, input_length=max_len))\r\n",
        "lstm.add(LSTM(256, return_sequences=True))\r\n",
        "lstm.add(Dropout(0.25))\r\n",
        "lstm.add(Bidirectional(LSTM(128)))\r\n",
        "lstm.add(Dropout(0.25))\r\n",
        "lstm.add(Dense(1, activation='softmax'))\r\n",
        "\r\n",
        "# Compile the model\r\n",
        "lstm.compile(loss='categorical_crossentropy',\r\n",
        "             optimizer='sgd',\r\n",
        "             metrics='accuracy')\r\n",
        "\r\n",
        "# Look at the summary of the model\r\n",
        "lstm.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 125, 64)           1982720   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 125, 256)          328704    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 125, 256)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 256)               394240    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 2,705,921\n",
            "Trainable params: 2,705,921\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcWXx9degtAK",
        "outputId": "e3196635-b7b2-41b2-ae9e-58820278e579"
      },
      "source": [
        "# Fit my model\r\n",
        "lstm1 = lstm.fit(X_train, y_train,\r\n",
        "                 batch_size=64,\r\n",
        "                 epochs=10,\r\n",
        "                 validation_data=(X_test, y_test))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "141/141 [==============================] - 5s 38ms/step - loss: 0.0000e+00 - accuracy: 0.0499 - val_loss: 0.0000e+00 - val_accuracy: 0.0396\n",
            "Epoch 2/10\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.0000e+00 - accuracy: 0.0499 - val_loss: 0.0000e+00 - val_accuracy: 0.0396\n",
            "Epoch 3/10\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.0000e+00 - accuracy: 0.0499 - val_loss: 0.0000e+00 - val_accuracy: 0.0396\n",
            "Epoch 4/10\n",
            "141/141 [==============================] - 4s 32ms/step - loss: 0.0000e+00 - accuracy: 0.0499 - val_loss: 0.0000e+00 - val_accuracy: 0.0396\n",
            "Epoch 5/10\n",
            "141/141 [==============================] - 4s 32ms/step - loss: 0.0000e+00 - accuracy: 0.0499 - val_loss: 0.0000e+00 - val_accuracy: 0.0396\n",
            "Epoch 6/10\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.0000e+00 - accuracy: 0.0499 - val_loss: 0.0000e+00 - val_accuracy: 0.0396\n",
            "Epoch 7/10\n",
            "141/141 [==============================] - 4s 32ms/step - loss: 0.0000e+00 - accuracy: 0.0499 - val_loss: 0.0000e+00 - val_accuracy: 0.0396\n",
            "Epoch 8/10\n",
            "141/141 [==============================] - 4s 32ms/step - loss: 0.0000e+00 - accuracy: 0.0499 - val_loss: 0.0000e+00 - val_accuracy: 0.0396\n",
            "Epoch 9/10\n",
            "141/141 [==============================] - 4s 32ms/step - loss: 0.0000e+00 - accuracy: 0.0499 - val_loss: 0.0000e+00 - val_accuracy: 0.0396\n",
            "Epoch 10/10\n",
            "141/141 [==============================] - 4s 32ms/step - loss: 0.0000e+00 - accuracy: 0.0499 - val_loss: 0.0000e+00 - val_accuracy: 0.0396\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "Euo0pIj5ePS_"
      },
      "source": [
        "## Sequence Data Question\n",
        "#### *Describe the `pad_sequences` method used on the training dataset. What does it do? Why do you need it?*\n",
        "\n",
        "  - Pad sequences adds 0s to either the beginning (by default) or end of a sequence to ensure that all sequences in a list have the same length. \n",
        "\n",
        "\n",
        "## RNNs versus LSTMs\n",
        "#### *What are the primary motivations behind using Long-ShortTerm Memory Cell unit over traditional Recurrent Neural Networks?*\n",
        "\n",
        "  - As the RNN goes through the layers it losses it's \"memory\" of prior values that could still be of importance, while a LSTM maintains this memory, which in turn trains the model better.\n",
        "\n",
        "## RNN / LSTM Use Cases\n",
        "#### *Name and Describe 3 Use Cases of LSTMs or RNNs and why they are suited to that use case*\n",
        "\n",
        "  1. Weather Forcasting - LSTM would be best for this case because you are going to want to use data from at least a year ago to help you forecast better based on the time of year and what the average weather is like.\n",
        "\n",
        "  2. Stock Values Forcasting - In order to get the best predictive values you would want to use LSTM unless it is a newer stock. With stock values though, the more data you can train on, the closer you could get to predicting what that stock will do based on it's historical data.\n",
        "\n",
        "  3. Predicting Text - This one could be done with RNN or LSTM on a use case basis. If you have small sequences then using the RNN would work fine, but if you have large sequences, you will want to use the LSTM since the older a value is, the RNN will not be able to retain the memory of that value, even though it is valuable to predicting the future values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz0LCZd_O4IG"
      },
      "source": [
        "<a id=\"p2\"></a>\n",
        "## Part 2- CNNs\n",
        "\n",
        "### Find the Frog\n",
        "\n",
        "Time to play \"find the frog!\" Use Keras and [ResNet50v2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet_v2) (pre-trained) to detect which of the images with the `frog_images` subdirectory has a frog in it. Note: You will need to upload the images to Colab. \n",
        "\n",
        "<img align=\"left\" src=\"https://d3i6fh83elv35t.cloudfront.net/newshour/app/uploads/2017/03/GettyImages-654745934-1024x687.jpg\" width=400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8oAGyW6ePS_"
      },
      "source": [
        "The skimage function below will help you read in all the frog images into memory at once. You should use the preprocessing functions that come with ResnetV2, and you should also resize the images using scikit-image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whIqEWR236Af"
      },
      "source": [
        "from skimage.io import imread_collection\n",
        "\n",
        "images = imread_collection('./frog_images/*.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKnnnM8k38sN"
      },
      "source": [
        "print(type(images))\n",
        "print(type(images[0]), end=\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si5YfNqS50QU"
      },
      "source": [
        "Your goal is to validly run ResNet50v2 on the input images - don't worry about tuning or improving the model. Print out the predictions in any way you see fit. \n",
        "\n",
        "*Hint* - ResNet 50v2 doesn't just return \"frog\". The three labels it has for frogs are: `bullfrog, tree frog, tailed frog`\n",
        "\n",
        "*Stretch goals:* \n",
        "- Check for other things such as fish.\n",
        "- Print out the image with its predicted label\n",
        "- Wrap everything nicely in well documented fucntions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaT07ddW3nHz"
      },
      "source": [
        "from tensorflow.keras.applications.resnet_v2 import ResNet50V2, decode_predictions, preprocess_input\n",
        "# TODO - your code!\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEuhvSu7O5Rf"
      },
      "source": [
        "<a id=\"p3\"></a>\n",
        "## Part 3 - Autoencoders\n",
        "\n",
        "Describe a use case for an autoencoder given that an autoencoder tries to predict its own input. \n",
        "\n",
        "__*Your Answer:*__ \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "626zYgjkO7Vq"
      },
      "source": [
        "<a id=\"p4\"></a>\n",
        "## Part 4 - More..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__lDWfcUO8oo"
      },
      "source": [
        "Answer the following questions, with a target audience of a fellow Data Scientist:\n",
        "\n",
        "- What do you consider your strongest area, as a Data Scientist?\n",
        "- What area of Data Science would you most like to learn more about, and why?\n",
        "- Where do you think Data Science will be in 5 years?\n",
        "- What are the threats posed by AI to our society?\n",
        "- How do you think we can counteract those threats? \n",
        "- Do you think achieving General Artifical Intelligence is ever possible?\n",
        "\n",
        "A few sentences per answer is fine - only elaborate if time allows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Hoqe3mM_Mtc"
      },
      "source": [
        "## Congratulations! \n",
        "\n",
        "Thank you for your hard work, and congratulations! You've learned a lot, and you should proudly call yourself a Data Scientist.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF48QyuQePTD"
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\"\"\"<iframe src=\"https://giphy.com/embed/26xivLqkv86uJzqWk\" width=\"480\" height=\"270\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe><p><a href=\"https://giphy.com/gifs/mumm-champagne-saber-26xivLqkv86uJzqWk\">via GIPHY</a></p>\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}